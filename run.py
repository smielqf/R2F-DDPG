import numpy as np
import torch


from agent import DDPGAgentTrainer, Partial_DDPG_Transition
import autograd_hacks

import random
import matplotlib.pyplot as plt
import os
import argparse


from tqdm import tqdm

from plot import plot_all
from utils import make_env, project, reverse_flat_grad, geometric_median, geometry_median_aggregation, aggregate_grads


def byzantine_grads(trainers, malic=None, attack='sign-flip', robust_aggregate='mean', device='cpu', is_project=False, scaleorth=.1):
    """Apply Byzantine attacks on the gradients of critics, and perform different aggregation rules.

    Parameters
    ----------
    trainers : torch.nn.Module
        Agents for training.
    malic : List[int], optional
        The list of Byzantine agents, by default None
    attack : str, optional
        The type of Byzantine attacks, by default 'sign-flip'
    robust_aggregate : str, optional
        The aggregation rule, by default 'mean'
    device : str, optional
        The device for running models, by default 'cpu'
    is_project : bool, optional
        Whether using projection or not, by default False
    scaleorth : float, optional
        The length of the vector generated by Orthogonal attacks, by default .1
    """
    def _cal_angle_distance(grad_dict, num_agent, mac_list):
        grad_list = [[] for _ in range(num_agent)]
        for para_name in grad_dict:
            for i, grad in enumerate(grad_dict[para_name]):
                grad_list[i].append(grad.view(-1))
        grad_vec_list = []
        for grad in grad_list:
            grad_vec = torch.cat(grad).unsqueeze(dim=0)
            grad_vec_list.append(grad_vec)

        grad_vec_matrix = torch.cat(grad_vec_list, dim=0)
        t_grad_vec_matrix = torch.clone(grad_vec_matrix).detach()
        for m in mac_list:
            t_grad_vec_matrix[i] -= t_grad_vec_matrix[i]
        ave_grad_vec = torch.mean(t_grad_vec_matrix, dim=0) * (num_agent / (num_agent - len(mac_list)))
        med_grad_vec = torch.median(grad_vec_matrix, dim=0)[0]
        geo_grad_vec = geometric_median(grad_vec_matrix)

        med_ave_angle = torch.sum(torch.mul(ave_grad_vec, med_grad_vec)) / (torch.norm(ave_grad_vec) * torch.norm(med_grad_vec))
        geo_ave_angle = torch.sum(torch.mul(ave_grad_vec, geo_grad_vec)) / (torch.norm(ave_grad_vec) * torch.norm(geo_grad_vec))
        med_ave_distance = torch.norm(ave_grad_vec - med_grad_vec)
        geo_ave_distance = torch.norm(ave_grad_vec - geo_grad_vec)

        return med_ave_distance, geo_ave_distance, med_ave_angle , geo_ave_angle

    q_network_state_dict = trainers[0].q_network.state_dict()

    temp_grads = {}
    if robust_aggregate == 'mean':
        for para_name in q_network_state_dict:
            para = q_network_state_dict[para_name]
            temp_grads[para_name] = torch.zeros(size=para.shape, device=device, dtype=torch.float)
    else:
        for para_name in q_network_state_dict:
            temp_grads[para_name] = []
    
    #### Byzantine Attacks
    for i, agent in enumerate(trainers):
        gradient_dict = {k:v.grad for k, v in zip(agent.q_network.state_dict(), agent.q_network.parameters())}
        for para_name in gradient_dict:
            if robust_aggregate == 'mean':
                if malic is not None and i in malic:
                    if attack == 'sign-flip':
                        temp_grads[para_name] -= 10. * gradient_dict[para_name]
                    elif attack == 'same':
                        temp_grads[para_name] += 2. * torch.ones_like(gradient_dict[para_name]).to(device)
                    elif attack == 'gaussian':
                        temp_grads[para_name] += torch.normal(0, 100, size=gradient_dict[para_name].shape).to(device)
                else:
                    temp_grads[para_name] += gradient_dict[para_name]
            else:
                if malic is not None and i in malic:
                    if attack == 'sign-flip':
                        temp_grads[para_name].append(-10. * gradient_dict[para_name])
                    elif attack == 'same':
                        temp_grads[para_name].append(2. * torch.ones_like(gradient_dict[para_name]).to(device))
                    elif attack == 'gaussian':
                        temp_grads[para_name].append(torch.normal(2, 100, size=gradient_dict[para_name].shape).to(device))
                else:
                    temp_grads[para_name].append(gradient_dict[para_name])   

    for i, agent in enumerate(trainers):
        if robust_aggregate == 'mean':
            if malic is not None and i in malic:
                if attack == 'orthogonal':
                    flat_rand_grad = torch.ones_like(agent.full_grads[0]).to(device)
                    proj_flat_rand_grad = project(agent.full_grads, flat_rand_grad)
                    flat_orth_grad = flat_rand_grad - proj_flat_rand_grad
                    flat_orth_grad = flat_orth_grad / torch.linalg.norm(flat_orth_grad) * scaleorth
                    new_grad =  reverse_flat_grad(gradient_dict, flat_orth_grad)
                    for para_name in new_grad:
                        temp_grads[para_name] += new_grad[para_name]
        else:
            if malic is not None and i in malic:
                if attack == 'orthogonal':
                    flat_rand_grad = torch.ones_like(agent.full_grads[0]).to(device)
                    proj_flat_rand_grad = project(agent.full_grads, flat_rand_grad)
                    flat_orth_grad = flat_rand_grad - proj_flat_rand_grad
                    flat_orth_grad = flat_orth_grad / torch.linalg.norm(flat_orth_grad) * scaleorth
                    new_grad =  reverse_flat_grad(gradient_dict, flat_orth_grad)
                    for para_name in new_grad:
                        temp_grads[para_name].append(new_grad[para_name])       

    
    ### Aggregation 
    if  robust_aggregate == 'mean':
        med_ave_distance, geo_ave_distance, med_ave_angle , geo_ave_angle = (.0, .0, .0, .0)
        for para_name in temp_grads:
            temp_grads[para_name] /= len(trainers)
    elif robust_aggregate  == 'median':
        med_ave_distance, geo_ave_distance, med_ave_angle , geo_ave_angle = _cal_angle_distance(temp_grads, len(trainers), malic)
        for para_name in temp_grads:
            all_paras = torch.stack(temp_grads[para_name], dim=0)
            temp_grads[para_name] = torch.median(all_paras, dim=0)[0]
    elif robust_aggregate == 'geometry_median':
        med_ave_distance, geo_ave_distance, med_ave_angle , geo_ave_angle = _cal_angle_distance(temp_grads, len(trainers), malic)
        temp_grads = geometry_median_aggregation(temp_grads, num_agent=len(trainers))
    
    if is_project:
        flat_grad = []
        for grad_name in temp_grads:
            flat_grad.append(temp_grads[grad_name].view(-1))
        flat_grad = torch.cat(flat_grad)

    for i, agent in enumerate(trainers):
        if is_project:
            project_grad = project(agent.full_grads, flat_grad)
            new_grads = reverse_flat_grad(temp_grads, project_grad)
        else:
            new_grads = temp_grads
        gradient_dict = {k:v for k, v in zip(agent.q_network.state_dict(), agent.q_network.parameters())}
        for para_name in gradient_dict:
            gradient_dict[para_name].grad.copy_(new_grads[para_name])

    
    return med_ave_distance, geo_ave_distance, med_ave_angle , geo_ave_angle


def cal_distance_angle(trainers):
    """Caculate the distance and the angle between the output of the aggregation rule and the true average gradient.

    Parameters
    ----------
    trainers : torch.nn.Module
        Agents for training.

    Returns
    -------
    List[float]
        Return the distance and the angle of GeoMed and CooMed.
    """
    q_network_state_dict = trainers[0].q_network.state_dict()
    grad_list = [[] for _ in range(len(trainers))]
    for para_name in q_network_state_dict:
        for i, agent in enumerate(trainers):
            gradient_dict = {k:v.grad for k, v in zip(agent.q_network.state_dict(), agent.q_network.parameters())}
            grad_list[i].append(gradient_dict[para_name].view(-1))
    grad_vec_list = []
    for grad in grad_list:
        grad_vec = torch.cat(grad).unsqueeze(dim=0)
        grad_vec_list.append(grad_vec)
    grad_vec_matrix = torch.cat(grad_vec_list, dim=0)
    ave_grad_vec = torch.mean(grad_vec_matrix, dim=0)
    med_grad_vec = torch.median(grad_vec_matrix, dim=0)[0]
    geo_grad_vec = geometric_median(grad_vec_matrix)

    med_ave_angle = torch.sum(torch.mul(ave_grad_vec, med_grad_vec)) / (torch.norm(ave_grad_vec) * torch.norm(med_grad_vec))
    geo_ave_angle = torch.sum(torch.mul(ave_grad_vec, geo_grad_vec)) / (torch.norm(ave_grad_vec) * torch.norm(geo_grad_vec))
    med_ave_distance = torch.norm(ave_grad_vec - med_grad_vec)
    geo_ave_distance = torch.norm(ave_grad_vec - geo_grad_vec)

    return med_ave_distance, geo_ave_distance, med_ave_angle , geo_ave_angle



def main(args, seed=0, malic_list=None, root_path=None):
    """Run the main function.

    Parameters
    ----------
    args : Parser
        Contain hyperparameters for training.
    seed : int, optional
        Random seed, by default 0
    malic_list : List[int], optional
        The list of Byzantien agents, by default None
    root_path : str, optional
        The path for saving results, by default None
    """
    env = make_env(args.scenario, args)
    num_agent = env.n
    num_episodes = args.num_episodes
    num_step_per_episode = args.max_episode_len
    

    batch_update = True
    if batch_update:
        update_gap = args.update_gap
    

    obs_n, partial_obs_n = env.reset()
    dim_input = len(partial_obs_n[0]) 
    dim_output = 5
    dim_critic_input = len(obs_n) + dim_output * num_agent
    print('dim_critic: {}'.format(dim_critic_input))
    trainers = [DDPGAgentTrainer(dim_input, dim_output, dim_critic_input, args,  agent_index=i, device=args.device, buffer_size=arglist.replay_buffer_size, partial=True) for i in range(num_agent) ]
    
    for agent in trainers:
        autograd_hacks.add_hooks(agent.q_network)
    
    for i, agent in enumerate(trainers):
        if i == 0:
            pass    ### Agent 0 represents the central server.
        else:
            agent.p_network.load_state_dict(trainers[0].p_network.state_dict())
            agent.q_network.load_state_dict(trainers[0].q_network.state_dict())
            agent.target_p_network.load_state_dict(trainers[0].target_p_network.state_dict())
            agent.target_q_network.load_state_dict(trainers[0].target_q_network.state_dict())
    
    

    total_rewards = [[] for _ in range(num_agent)]
    actor_losses = [[] for _ in range(num_agent)]
    critic_losses = [[] for _ in range(num_agent)]
    q_values = [[] for _ in range(num_agent)]

    count = 0

    med_ave_angle_list = []
    geo_ave_angle_list = []
    med_ave_distance_list = []
    geo_ave_distance_list = []
    
    for episode in tqdm(range(num_episodes)):
        actor_loss_episode = [[] for _ in range(num_agent)]
        critic_loss_episode = [[] for _ in range(num_agent)]
        rewards_episode = [[] for _ in range(num_agent)]
        obs_n, partial_obs_n = env.reset()
        q_value_episode = [[] for _ in range(num_agent)]
        rank_change_episode = []
        
        for step in range(num_step_per_episode):
        # Get action   
            action_n = [agent.act(partial_obs_n[i]) for i, agent in enumerate(trainers)]
            
            new_obs_n, new_partial_obs_n, reward_n, _, _ = env.step(action_n)

            # Collect experience for replay buffer
            if args.baseline:
                ave_reward = np.mean(reward_n)
                for i, agent in enumerate(trainers):
                    agent.replay_buffer.push(obs_n, partial_obs_n[i], action_n, ave_reward, new_obs_n, new_partial_obs_n[i])
            else:
                for i, agent in enumerate(trainers):
                    agent.replay_buffer.push(obs_n, partial_obs_n[i], action_n, reward_n[i], new_obs_n, new_partial_obs_n[i])

            if batch_update:
                if len(trainers[0].replay_buffer) < trainers[0].max_replay_buffer_len:
                    batch_index = None
                    for i in range(num_agent):
                        actor_loss_episode[i].append(.0)
                        critic_loss_episode[i].append(.0)
                        q_value_episode[i].append(.0)
                        rewards_episode[i].append(reward_n[i])
                else:
                    buffer_len = len(trainers[0].replay_buffer)
                    batch_index = random.sample(range(buffer_len), trainers[0].batch_size)
                    
                    act_next = []
                    for i, agent in enumerate(trainers):
                        transitions = []
                        for index in batch_index:
                            transitions.append(trainers[i].replay_buffer.buffer[index])
                        batch_data = Partial_DDPG_Transition(*zip(*transitions))
                        _obs_next = torch.tensor(batch_data.partial_next_state, device=args.device, dtype=torch.float, requires_grad=False)
                        _act_next = agent._get_action(agent.target_p_network, _obs_next).clone().cpu().detach().numpy()
                        act_next.append(_act_next)
                    act_next = torch.tensor(act_next, device=args.device, dtype=torch.float, requires_grad=False)
                    act_next = act_next.permute(1,0,2)
                        
                        
                    for i, agent in enumerate(trainers):
                        critic_loss, q_value = agent.batch_update_fully_q_train(update_gap, batch_index, partial=True, act_next=act_next)
                        critic_loss_episode[i].append(critic_loss)
                        q_value_episode[i].append(q_value)
                        rewards_episode[i].append(reward_n[i])
                    if critic_loss !=0:
                        if args.attack_mode == 'normal':
                            med_ave_distance, geo_ave_distance, med_ave_angle , geo_ave_angle = cal_distance_angle(trainers)
                            med_ave_angle_list.append(med_ave_angle.cpu().numpy())
                            geo_ave_angle_list.append(geo_ave_angle.cpu().numpy())
                            med_ave_distance_list.append(med_ave_distance.cpu().numpy())
                            geo_ave_distance_list.append(geo_ave_distance.cpu().numpy())
                            if not args.baseline:
                                rank_change = aggregate_grads(trainers, args.device, args.project)
                        else:
                            med_ave_distance, geo_ave_distance, med_ave_angle , geo_ave_angle = byzantine_grads(trainers, malic_list, args.attack_mode, args.robust_mode, args.device, args.project, args.scaleorth)
                            if med_ave_angle != .0:
                                med_ave_angle_list.append(med_ave_angle.cpu().numpy())
                                geo_ave_angle_list.append(geo_ave_angle.cpu().numpy())
                                med_ave_distance_list.append(med_ave_distance.cpu().numpy())
                                geo_ave_distance_list.append(geo_ave_distance.cpu().numpy())
                        # byzantine_grads(trainers, None, 'sign-flip', 'median')
                        # zero_attack_grads(trainers, malic_list)
                        for i, agent in enumerate(trainers):
                            agent.q_optimizer.step()
                            agent.make_update_exp(agent.q_network, agent.target_q_network)

                        

                        for i, agent in enumerate(trainers):
                            actor_loss = agent.batch_update_fully_p_train(update_gap, batch_index, partial=True)                        
                            
                            actor_loss_episode[i].append(actor_loss)

                    else:
                        for i, agent in enumerate(trainers):
                            actor_loss_episode[i].append(.0)
            else:
                actor_loss, critic_loss, q_value = trainers[0].single_update(new_obs_n[0], action_n[0], reward_n[0], new_obs_n[0])
            obs_n = new_obs_n
            partial_obs_n = new_partial_obs_n
        for i in range(num_agent):
            total_rewards[i].append(np.sum(rewards_episode[i]))
            actor_losses[i].append(np.mean(actor_loss_episode[i]))
            critic_losses[i].append(np.mean(critic_loss_episode[i]))
            q_values[i].append(np.mean(q_value_episode[i]))
        
    for agent in trainers:
        autograd_hacks.disable_hooks()
    if root_path is None:
        root_path = os.path.join(args.fig_path, 'partial_agent_{}'.format(num_agent))
    if not os.path.exists(root_path):
        os.mkdir(root_path)
    seed_path = os.path.join(root_path, 'seed_{}'.format(seed))
    if not os.path.exists(seed_path):
        os.mkdir(seed_path)
    attack_mode_path = os.path.join(seed_path, args.attack_mode)
    if not os.path.exists(attack_mode_path):
        os.mkdir(attack_mode_path)
    num_malicious_path = os.path.join(attack_mode_path, "num_malic_{}".format(args.num_malicious))
    if not os.path.exists(num_malicious_path):
        os.mkdir(num_malicious_path)
    robust_mode_path = os.path.join(num_malicious_path, args.robust_mode)
    if not os.path.exists(robust_mode_path):
        os.mkdir(robust_mode_path)
    
    if len(med_ave_angle_list) > 0:
        med_ave_angle = np.array(med_ave_angle_list)
        geo_ave_angle = np.array(geo_ave_angle_list)
        med_ave_distance = np.array(med_ave_distance_list)
        geo_ave_distance = np.array(geo_ave_distance_list)
        plt.plot([np.mean(med_ave_angle[:j]) for j in range(1, len(med_ave_angle)+1)], label='median')
        plt.plot([np.mean(geo_ave_angle[:j]) for j in range(1, len(geo_ave_angle)+1)], label='geometric median')
        plt.title('angle')
        plt.legend()
        filename = os.path.join(robust_mode_path, 'angle.png')
        plt.savefig(filename)
        plt.clf()
        plt.plot([np.mean(med_ave_distance[:j]) for j in range(1, len(med_ave_distance)+1)], label='median')
        plt.plot([np.mean(geo_ave_distance[:j]) for j in range(1, len(geo_ave_distance)+1)], label='geometric median')
        plt.title('distance')
        plt.legend()
        filename = os.path.join(robust_mode_path, 'distance.png')
        plt.savefig(filename)


    np.save(os.path.join(robust_mode_path, "total_rewards.npy"), np.array(total_rewards))
    np.save(os.path.join(robust_mode_path, "actor_losses.npy"), np.array(actor_losses))
    np.save(os.path.join(robust_mode_path, "critic_losses.npy"), np.array(critic_losses))
    np.save(os.path.join(robust_mode_path, "q_values.npy"), np.array(q_values))
    np.save(os.path.join(robust_mode_path, "med_ave_angle.npy"), np.array(med_ave_angle_list))
    np.save(os.path.join(robust_mode_path, "geo_ave_angle.npy"), np.array(geo_ave_angle_list))
    np.save(os.path.join(robust_mode_path, "med_ave_distance.npy"), np.array(med_ave_distance_list))
    np.save(os.path.join(robust_mode_path, "geo_ave_distance.npy"), np.array(geo_ave_distance_list))
    
    plot_all(robust_mode_path, num_agent, update_gap)


def parse_args():
    parser = argparse.ArgumentParser("R2F-DDPG")

    parser.add_argument("--baseline", action='store_true', help="Run baseline MADDPG")
    parser.add_argument("--scenario", type=str, default="spread_fiveagent_no_diff_dist", help='environment')
    parser.add_argument("--partial", action='store_true', help='Partially observable')
    parser.add_argument('--diff', action='store_true', help='diff observation')
    parser.add_argument("--num-agent", type=int, default=5, help='number of agents')

    parser.add_argument("--update-gap", type=int, default=10, help='frequency for update')
    parser.add_argument("--batch-size", type=int, default=100)
    parser.add_argument("--max-episode-len", type=int, default=25)
    parser.add_argument("--num-episodes", type=int, default=2000)
    parser.add_argument("--replay-buffer-size", type=int, default=100000)
    parser.add_argument("--aggr-model", action='store_true', help='aggregate model of agents')
    parser.add_argument('--project', action='store_true', help='projection')

    parser.add_argument("--num-malicious", type=int, default=0)
    parser.add_argument("--attack-mode", type=str, default="normal", help="the method used for attacks")
    parser.add_argument("--robust-mode", type=str, default='mean', help='the method used for roubuts aggregation')
    parser.add_argument("--scaleorth", type=float, default=.1)

    parser.add_argument("--device", type=str, default='cpu')

    parser.add_argument("--fig-path", type=str, default="./figures", help="directory in which training state and model should be saved")
    parser.add_argument("--root-path", type=str, default=None, help="root directory in which figures should be saved")
    parser.add_argument("--plot", action="store_true", default=False)



    return parser.parse_args()
if __name__ == '__main__':

    for seed in tqdm(range(5)):
        np.random.seed(0)
        arglist = parse_args()
        malic_list = np.random.choice(range(arglist.num_agent), size=arglist.num_malicious)
        print("malic_list:{}".format(malic_list))
        np.random.seed(seed)
        torch.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        random.seed(seed)


        if arglist.root_path is not None:
            main(arglist, seed, malic_list, os.path.join(arglist.fig_path, arglist.root_path))
        else:
            main(arglist, seed, malic_list)


